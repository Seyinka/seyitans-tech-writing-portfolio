# Model Card — LLaMA 2

## Model Details
- **Name:** LLaMA 2
- **Developer:** Meta AI
- **Release Date:** July 2023
- **Parameter Sizes:** 7B, 13B, 70B 
- **Variations:** Pretrained and Fine-tuned
- **Input Models:** Text Only
- **Output Models:** Text Only



## Purpose
- **Uses:** For commercial and research, chatbots, code assistants, and educational use.
- **Not for:** Disallowed content that violates applicable, laws and regulations(as per license agreement).

## Training Data
- Llama 2 is pretrained on about 2 trillion tokens of data from publicly available sources.
- Its fined-tuned data includes publicly available instruction datasets, as well as over one million new human-annotated examples.
-  No Meta user data included for neither the pretrained or fine-tuned datasets.
-  Gaps Filled : 

## Metrics
- Overall performance on grouped academic benchmarks : 
  Code, Commonsense Reasoning, World Knowledge, Reading Comprehension, Math.
- Evaluation of pretrained LLMs on automatic safety benchmarks :
  
  - TruthfulQA : presents the percentage of an AI's answers that are both truthful and informative.

  - ToxiGen : presents how much an AI's responses are toxic.
- Evaluation of fine-tuned LLMs on different safety datasets.
- Gaps Filled : 

## Risks
- The risks identified are that Llama 2 can sometimes give inaccurate, biased, or inappropriate responses. Since it hasn’t been tested in all languages or scenarios, its behavior can’t always be predicted, so developers must do safety checks before using it in real applications.
- Gaps Filled : 

## 
